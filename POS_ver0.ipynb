{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS-ver0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP1UHkYcOdM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install flair==0.4.2 \n",
        "\n",
        "# from typing import List\n",
        "# from flair.embeddings import FlairEmbeddings, TokenEmbeddings, StackedEmbeddings\n",
        "# from flair.training_utils import EvaluationMetric\n",
        "# from flair.data import Corpus\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pML_0F-XO91E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #DATASET\n",
        "\n",
        "# # # 1. get the corpora\n",
        "# from flair.datasets import UD_ENGLISH\n",
        "# corpus1: Corpus = UD_ENGLISH().downsample(0.1)\n",
        "# print(type(corpus1.train))\n",
        "# print(type(corpus1.train[0]))\n",
        "# print(corpus1.train[0].to_tagged_string('pos'))\n",
        "# print(corpus1.train[1].to_tagged_string('pos'))\n",
        "\n",
        "# from flair.data import Corpus\n",
        "# from flair.datasets import ColumnCorpus\n",
        "\n",
        "# # define columns\n",
        "# columns = {0: 'text', 1: 'pos'}\n",
        "\n",
        "# # this is the folder in which train, test and dev files reside\n",
        "# data_folder =   ''  #'/path/to/data/folder'\n",
        "\n",
        "# # init a corpus using column format, data folder and the names of the train, dev and test files\n",
        "# corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "#                               train_file='fulldata_train.txt',\n",
        "#                               test_file='fulldata_test.txt',\n",
        "#                               dev_file='fulldata_dev.txt')\n",
        "\n",
        "# print(type(corpus.train))\n",
        "# print(type(corpus.train[0]))\n",
        "# print(corpus.train[0].to_tagged_string('pos'))\n",
        "# print(corpus.train[1].to_tagged_string('pos'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fzoppSUPaYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tag_dictionary = corpus.make_tag_dictionary(tag_type='pos')\n",
        "# print(tag_dictionary.idx2item)\n",
        "# print(tag_dictionary.item2idx)\n",
        "# print(tag_dictionary.multi_label)\n",
        "# # print(tag_dictionary.get_idx_for_item('AUX'))\n",
        "# # print(tag_dictionary.get_item_for_index(6))\n",
        "\n",
        "\n",
        "# tag_dictionary = corpus1.make_tag_dictionary(tag_type='upos')\n",
        "# print(tag_dictionary.idx2item)\n",
        "# print(tag_dictionary.item2idx)\n",
        "# print(tag_dictionary.multi_label)\n",
        "# print(tag_dictionary.get_idx_for_item('AUX'))\n",
        "# print(tag_dictionary.get_item_for_index(6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rodk1AtRWQZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from flair.models import SequenceTagger\n",
        "\n",
        "\n",
        "# tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "#                                         embeddings=embeddings,\n",
        "#                                         tag_dictionary=tag_dictionary,\n",
        "#                                         tag_type='pos',\n",
        "#                                         use_crf=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIyWZEzVTT0R",
        "colab_type": "text"
      },
      "source": [
        "# **EVALUATION TnT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIYZX_aoNSqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import csv\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_cnvZY4TjAG",
        "colab_type": "code",
        "outputId": "5842ec1a-4dba-4054-c0b3-057ece6402b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data1 = pd.read_csv('TED_Data.csv')\n",
        "data1 = data1[['Words','Tags']]\n",
        "data1 = data1.dropna()\n",
        "\n",
        "data2 = pd.read_csv('BNC_Data.csv')\n",
        "data2 = data2[['Words','Tags']]\n",
        "data2 = data2.dropna()\n",
        "\n",
        "data3 = pd.read_csv('ANC_Data1.csv')\n",
        "data3 = data3[['Words','Tags']]\n",
        "data3 = data3.dropna()\n",
        "\n",
        "#-----------------------\n",
        "\n",
        "d1_tuples = []\n",
        "sentence = []\n",
        "for i in range(len(data1)):\n",
        "  x = (data1.iloc[i]['Words'] ,data1.iloc[i]['Tags'])\n",
        "  sentence.append(x)\n",
        "\n",
        "  #SENTENCES LONGER THAN 1000 WORDS WERE REMOVED\n",
        "  #THERE WERE INITAILLY 6K SENTENCES.. NOW THE DATA SET IS REDUCE TO HALF\n",
        "\n",
        "  #Average sentence length 30-60. \n",
        "  if(data1.iloc[i]['Tags'] == '.'):\n",
        "    if( len(sentence) > 1500):      \n",
        "      continue \n",
        "    d1_tuples.append(sentence)\n",
        "    sentence = []\n",
        "\n",
        "random.shuffle(d1_tuples)\n",
        "\n",
        "#----------------------\n",
        "\n",
        "d2_tuples = []\n",
        "sentence = []\n",
        "for i in range(len(data2)):\n",
        "  x = (data2.iloc[i]['Words'] ,data2.iloc[i]['Tags'])\n",
        "  sentence.append(x)\n",
        "\n",
        "  #SENTENCES LONGER THAN 1000 WORDS WERE REMOVED\n",
        "  #THERE WERE INITAILLY 6K SENTENCES.. NOW THE DATA SET IS REDUCE TO HALF\n",
        "\n",
        "  #Average sentence length 30-60. \n",
        "  if(data2.iloc[i]['Tags'] == '.'):\n",
        "    if( len(sentence) > 1500):      \n",
        "      continue \n",
        "    d2_tuples.append(sentence)\n",
        "    sentence = []\n",
        "\n",
        "random.shuffle(d2_tuples)\n",
        "\n",
        "#------------------------------\n",
        "\n",
        "d3_tuples = []\n",
        "sentence = []\n",
        "for i in range(len(data3)):\n",
        "  x = (data3.iloc[i]['Words'] ,data3.iloc[i]['Tags'])\n",
        "  sentence.append(x)\n",
        "\n",
        "  #SENTENCES LONGER THAN 1000 WORDS WERE REMOVED\n",
        "  #THERE WERE INITAILLY 6K SENTENCES.. NOW THE DATA SET IS REDUCE TO HALF\n",
        "\n",
        "  #Average sentence length 30-60. \n",
        "  if(data3.iloc[i]['Tags'] == '.'):\n",
        "    if( len(sentence) > 1500):      \n",
        "      continue \n",
        "    d3_tuples.append(sentence)\n",
        "    sentence = []\n",
        "\n",
        "random.shuffle(d3_tuples)\n",
        "\n",
        "\n",
        "#--------------------\n",
        "\n",
        "print(len(d1_tuples))\n",
        "print(len(d2_tuples))\n",
        "print(len(d3_tuples))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1272\n",
            "1002\n",
            "3387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcZ3_VUtVpJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DataSet = d1_tuples + d2_tuples + d3_tuples\n",
        "random.shuffle(DataSet)\n",
        "\n",
        "train = DataSet[:4246]\n",
        "test = DataSet[4246:]\n",
        "\n",
        "import pickle\n",
        "\n",
        "data = [train,test]\n",
        "with open('data.pickle', 'wb') as f:\n",
        "    pickle.dump(data, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzd3qbYzZZQ0",
        "colab_type": "code",
        "outputId": "183965b0-cb6b-450a-ff27-a6ff1ca24dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "from nltk.tag import tnt \n",
        "from nltk.tag import DefaultTagger \n",
        "\n",
        "#REAF PICKLE FILE\n",
        "# import pickle   ><><><><><><\n",
        "\n",
        "# [train,test] = pickle.load('data.pickle', )   <><><><><><><><\n",
        "  \n",
        "# # initializing tagger \n",
        "# # def __init__(self, unk=None, Trained=False, N=1000, C=False):    <- default \n",
        "\n",
        "unk = DefaultTagger('NN') \n",
        "tnt_tagging = tnt.TnT(unk = unk, Trained = True, N=100000 ,C=True ) \n",
        "\n",
        "# training \n",
        "tnt_tagging.train( train ) \n",
        "  \n",
        "# evaluating \n",
        "tagged = tnt_tagging.tagdata(test)\n",
        "# data = [train,test]\n",
        "with open('tagged_yhat_n10.pickle', 'wb') as f:\n",
        "    pickle.dump(tagged, f)\n",
        "\n",
        "\n",
        "a = tnt_tagging.evaluate(test) \n",
        "\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-babf4807a0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtnt_tagging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# evaluating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    }
  ]
}