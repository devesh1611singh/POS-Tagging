{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS-ver1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYVTWAtZP_az",
        "colab_type": "text"
      },
      "source": [
        "# Merging Datasets and Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIYZX_aoNSqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import csv\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8eMcj5LnuGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxsnD6wrm65P",
        "colab_type": "code",
        "outputId": "ea475384-ca8a-46d2-cfb4-ee3b150f078e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import sys\n",
        "limit = sys.getrecursionlimit() \n",
        "  \n",
        "# Print the current limit  \n",
        "print('Before changing, limit of stack =', limit)  \n",
        "\n",
        "Newlimit = limit*2\n",
        "  \n",
        "# Using sys.setrecursionlimit() method  \n",
        "sys.setrecursionlimit(Newlimit)  \n",
        "  \n",
        "# Using sys.getrecursionlimit() method  \n",
        "# to find the current recursion limit \n",
        "limit = sys.getrecursionlimit() \n",
        "  \n",
        "# Print the current limit  \n",
        "print('After changing, limit of stack =', limit)  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before changing, limit of stack = 4000\n",
            "After changing, limit of stack = 8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLAzNRQzaHbt",
        "colab_type": "code",
        "outputId": "79de0d0f-0097-4f60-d5c7-9c479d6dcb41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data1 = pd.read_csv('TED_Data.csv')\n",
        "data1 = data1[['Words','Tags']]\n",
        "data1 = data1.dropna()\n",
        "\n",
        "data2 = pd.read_csv('BNC_Data.csv')\n",
        "data2 = data2[['Words','Tags']]\n",
        "data2 = data2.dropna()\n",
        "\n",
        "data3 = pd.read_csv('ANC_Data1.csv')\n",
        "data3 = data3[['Words','Tags']]\n",
        "data3 = data3.dropna()\n",
        "\n",
        "#-----------------------\n",
        "\n",
        "d1_tuples = []\n",
        "sentence = []\n",
        "for i in range(len(data1)):\n",
        "  x = (data1.iloc[i]['Words'] ,data1.iloc[i]['Tags'])\n",
        "  sentence.append(x)\n",
        "\n",
        "  #SENTENCES LONGER THAN 1000 WORDS WERE REMOVED\n",
        "  #THERE WERE INITAILLY 6K SENTENCES.. NOW THE DATA SET IS REDUCE TO HALF\n",
        "\n",
        "  #Average sentence length 30-60. \n",
        "  if(data1.iloc[i]['Tags'] == '.'):\n",
        "    if( len(sentence) > 1500):      \n",
        "      continue \n",
        "    d1_tuples.append(sentence)\n",
        "    sentence = []\n",
        "\n",
        "random.shuffle(d1_tuples)\n",
        "\n",
        "#----------------------\n",
        "\n",
        "d2_tuples = []\n",
        "sentence = []\n",
        "for i in range(len(data2)):\n",
        "  x = (data2.iloc[i]['Words'] ,data2.iloc[i]['Tags'])\n",
        "  sentence.append(x)\n",
        "\n",
        "  #SENTENCES LONGER THAN 1000 WORDS WERE REMOVED\n",
        "  #THERE WERE INITAILLY 6K SENTENCES.. NOW THE DATA SET IS REDUCE TO HALF\n",
        "\n",
        "  #Average sentence length 30-60. \n",
        "  if(data2.iloc[i]['Tags'] == '.'):\n",
        "    if( len(sentence) > 1500):      \n",
        "      continue \n",
        "    d2_tuples.append(sentence)\n",
        "    sentence = []\n",
        "\n",
        "random.shuffle(d2_tuples)\n",
        "\n",
        "#------------------------------\n",
        "\n",
        "d3_tuples = []\n",
        "sentence = []\n",
        "for i in range(len(data3)):\n",
        "  x = (data3.iloc[i]['Words'] ,data3.iloc[i]['Tags'])\n",
        "  sentence.append(x)\n",
        "\n",
        "  #SENTENCES LONGER THAN 1000 WORDS WERE REMOVED\n",
        "  #THERE WERE INITAILLY 6K SENTENCES.. NOW THE DATA SET IS REDUCE TO HALF\n",
        "\n",
        "  #Average sentence length 30-60. \n",
        "  if(data3.iloc[i]['Tags'] == '.'):\n",
        "    if( len(sentence) > 1500):      \n",
        "      continue \n",
        "    d3_tuples.append(sentence)\n",
        "    sentence = []\n",
        "\n",
        "random.shuffle(d3_tuples)\n",
        "\n",
        "\n",
        "#--------------------\n",
        "\n",
        "print(len(d1_tuples))\n",
        "print(len(d2_tuples))\n",
        "print(len(d3_tuples))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1272\n",
            "1002\n",
            "3387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxDvz9aPvyIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DataSet = d1_tuples + d2_tuples + d3_tuples\n",
        "random.shuffle(DataSet)\n",
        "\n",
        "train = DataSet[:4246]\n",
        "dev = DataSet[4246:4925]\n",
        "test = DataSet[4925:]\n",
        "\n",
        "# data = [train, dev, test]\n",
        "\n",
        "\n",
        "# print(train[0])\n",
        "# print(train[0][0])\n",
        "# print(train[0][0][0])\n",
        "# print(type(train[0][0][0])  )\n",
        "\n",
        "# sentence = []\n",
        "# for i in range(len(train)):\n",
        "#   sentence = []\n",
        "#   for j in range(len(train[i])):  \n",
        "#     sentence.append(  train[i][j][0]  + ' ' + train[i][j][1]   ) \n",
        "#   sentence.append(' ')  \n",
        "\n",
        "#   with open('fulldata_train.txt', 'a+') as f:\n",
        "#     for item in sentence:\n",
        "#       f.write(\"%s\\n\" % item)\n",
        "\n",
        "sentence = []\n",
        "for i in range(len(train)-1):\n",
        "  sentence = []\n",
        "  for j in range(len(train[i])):  \n",
        "    sentence.append(  train[i][j][0]  + ' ' + train[i][j][1]   ) \n",
        "  sentence.append(' ')  \n",
        "  \n",
        "  with open('fulldata_train.txt', 'a+') as f:\n",
        "    for item in sentence:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "sentence = []\n",
        "for i in range(len(dev)-1):\n",
        "  sentence = []\n",
        "  for j in range(len(dev[i])):  \n",
        "    sentence.append(  dev[i][j][0]  + ' ' + dev[i][j][1]   ) \n",
        "  sentence.append(' ')  \n",
        "  \n",
        "  with open('fulldata_dev.txt', 'a+') as f:\n",
        "    for item in sentence:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "sentence = []\n",
        "for i in range(len(test)-1):\n",
        "  sentence = []\n",
        "  for j in range(len(test[i])):  \n",
        "    sentence.append(  test[i][j][0]  + ' ' + test[i][j][1]   ) \n",
        "  sentence.append(' ')  \n",
        "  \n",
        "  with open('fulldata_test.txt', 'a+') as f:\n",
        "    for item in sentence:\n",
        "      f.write(\"%s\\n\" % item)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQk9ur4Nlo_0",
        "colab_type": "text"
      },
      "source": [
        "# **TNT (HMM)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVY3EtNiTh2Q",
        "colab_type": "code",
        "outputId": "1cabae6f-5ff8-4e09-cdb7-ff0bf64c8a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "\n",
        "# train_tuples = []\n",
        "# sentence = []\n",
        "\n",
        "# for i in range(len(data23)):\n",
        "#   x = (data23.iloc[i]['Words'] ,data23.iloc[i]['Tags'])\n",
        "#   sentence.append(x)\n",
        "\n",
        "#   #SENTENCES LONGER THAN 1000 WORDS WERE REMOVED\n",
        "#   #THERE WERE INITAILLY 6K SENTENCES.. NOW THE DATA SET IS REDUCE TO HALF\n",
        "\n",
        "#   #Average sentence length 30-60. \n",
        "#   if(data23.iloc[i]['Tags'] == '.'):\n",
        "#     if( len(sentence) > 1500):      \n",
        "#       continue \n",
        "#     train_tuples.append(sentence)\n",
        "#     sentence = []\n",
        "\n",
        "# random.shuffle(train_tuples)\n",
        "\n",
        "# #--------------------------------------------\n",
        "\n",
        "# test_tuples = []\n",
        "# sentence = []\n",
        "# for i in range(len(data3)):\n",
        "#   x = (data3.iloc[i]['Words'] ,data3.iloc[i]['Tags'])\n",
        "#   sentence.append(x)\n",
        "\n",
        "#   #SENTENCES LONGER THAN 1000 WORDS WERE REMOVED\n",
        "#   #THERE WERE INITAILLY 6K SENTENCES.. NOW THE DATA SET IS REDUCE TO HALF\n",
        "\n",
        "#   #Average sentence length 30-60. \n",
        "#   if(data3.iloc[i]['Tags'] == '.'):\n",
        "#     if( len(sentence) > 1500):      \n",
        "#       continue \n",
        "#     test_tuples.append(sentence)\n",
        "#     sentence = []\n",
        "\n",
        "# random.shuffle(test_tuples)\n",
        "\n",
        "# print(len(train_tuples))\n",
        "# print(len(test_tuples))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2274\n",
            "3387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4p9ZBmbXjIz",
        "colab_type": "code",
        "outputId": "668a2e67-ebfc-4383-c236-fcc21c49ede7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#   TnT code (HMM)\n",
        "#\n",
        "\n",
        "from nltk.tag import tnt \n",
        "\n",
        "#75 percent training data; 25 percent test data\n",
        "# data splitting happens on sentence level; \n",
        "# BNC - 1002 sentences\n",
        "# ANC - 4307 sentences\n",
        "# TED - 1272 sentences\n",
        "\n",
        "#train_data = tuples[:2540]\n",
        "#test_data = tuples[2540:]\n",
        "\n",
        "\n",
        "train_tuples = d1_tuples + d3_tuples + d2_tuples\n",
        "random.shuffle(train_tuples)\n",
        "\n",
        "#test_tuples = d2_tuples\n",
        "\n",
        "# initializing tagger \n",
        "tnt_tagging = tnt.TnT() \n",
        "  \n",
        "# training \n",
        "tnt_tagging.train( train_tuples[:4246] ) \n",
        "  \n",
        "# evaluating \n",
        "a = tnt_tagging.evaluate(train_tuples[4246:]) \n",
        "\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8732768875516059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBiNJ0bouNmS",
        "colab_type": "text"
      },
      "source": [
        "    tagged_data = tagger.tagdata(test[100:120])\n",
        "\n",
        "Use this function, with (sklearn.mteric/evaluate) to compute the confusion matrix. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnXD4TQuwEYI",
        "colab_type": "text"
      },
      "source": [
        "ALSO EXPERIMENT WITH N=1000 and C=False, if get time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_21NsG0ldv2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **STANFORD Bicyclic dependency**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgtD1eoms1Ku",
        "colab_type": "code",
        "outputId": "af9adb9b-df72-4069-c5e3-ef3fab9aa021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "#from nltk.tag.stanford import CoreNLPNERTagger\n",
        "\n",
        "from nltk.parse import CoreNLPParser\n",
        "\n",
        "# Lexical Parser\n",
        "# parser = CoreNLPParser(url='http://localhost:9000')\n",
        "\n",
        "pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n",
        "print( list(pos_tagger.tag('What is the airspeed of an unladen swallow ?'.split())) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2227647debd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# parser = CoreNLPParser(url='http://localhost:9000')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpos_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://localhost:9000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'What is the airspeed of an unladen swallow ?'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'tagtype'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0e0wXRlsc0",
        "colab_type": "text"
      },
      "source": [
        "# **BILSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlFHIuaRlvUX",
        "colab_type": "code",
        "outputId": "3343a223-4a1b-47e1-a983-c5df2e1a71f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install flair==0.4.2 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair==0.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/3a/2e777f65a71c1eaa259df44c44e39d7071ba8c7780a1564316a38bf86449/flair-0.4.2-py3-none-any.whl (136kB)\n",
            "\r\u001b[K     |██▍                             | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 40kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 81kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 92kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 133kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (0.0)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (3.6.4)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (3.6.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (0.8.6)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (0.1.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (3.1.2)\n",
            "Collecting pytorch-pretrained-bert>=0.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 17.0MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 17.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (2019.12.20)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (1.3.1)\n",
            "Collecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.2) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair==0.4.2) (0.22.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.2) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.2) (42.0.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.2) (1.8.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.2) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.2) (8.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.2) (19.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.2) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.2) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.2) (1.9.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.2) (3.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.2) (2.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.2) (0.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.2) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.2) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.2) (2.6.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair==0.4.2) (1.10.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair==0.4.2) (2.21.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.4.2) (1.11.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 29.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair==0.4.2) (0.14.1)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.2) (2.49.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.4.2) (4.4.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair==0.4.2) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair==0.4.2) (1.13.47)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair==0.4.2) (0.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair==0.4.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair==0.4.2) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair==0.4.2) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert>=0.6.1->flair==0.4.2) (0.15.2)\n",
            "Building wheels for collected packages: sqlitedict, mpld3, segtok\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=23025e01b6386892292efa4042d70ad6bbfa49fa2ae690c7eeba9427039acef3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=9af5a94979e13a1975534806715239e21e96f0a9faadcdd6e39f91fbc9aac90d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=28ad7eafe7041a383b0e8ff1907bb28a6288aa597ce9c88de049fe8a8b03d676\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "Successfully built sqlitedict mpld3 segtok\n",
            "Installing collected packages: sqlitedict, pytorch-pretrained-bert, deprecated, mpld3, segtok, sentencepiece, bpemb, flair\n",
            "Successfully installed bpemb-0.3.0 deprecated-1.2.7 flair-0.4.2 mpld3-0.3 pytorch-pretrained-bert-0.6.2 segtok-1.5.7 sentencepiece-0.1.85 sqlitedict-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b6lV9_3HUU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mOaTH9SmTMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from flair.data import Sentence\n",
        "# from flair.models import SequenceTagger\n",
        "\n",
        "# import flair.datasets\n",
        "# corpus = flair.datasets.UD_ENGLISH()\n",
        "\n",
        "# # make a sentence\n",
        "# #sentence = Sentence('I love Berlin .')\n",
        "\n",
        "# sentence =  corpus.test[0]\n",
        "\n",
        "# # load the NER tagger\n",
        "# tagger = SequenceTagger.load('pos')\n",
        "\n",
        "# # run NER over sentence\n",
        "# tagger.predict(sentence)\n",
        "\n",
        "# # iterate over entities and print\n",
        "# for entity in sentence.get_spans('pos'):\n",
        "#     print(entity)\n",
        "#     print(entity.tag)     #<------- cool\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecvFWxcftzFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "from flair.embeddings import FlairEmbeddings, TokenEmbeddings, StackedEmbeddings\n",
        "# from flair.data import MultiCorpus\n",
        "from flair.datasets import UD_ENGLISH, UD_GERMAN\n",
        "from flair.training_utils import EvaluationMetric\n",
        "from flair.data import Corpus\n",
        "# from flair.datasets import WNUT_17\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN5qKyGUa5kv",
        "colab_type": "code",
        "outputId": "6875bf4f-1e1d-4df7-cbfa-d3dc4dea3eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#DATASET\n",
        "\n",
        "# # 1. get the corpora - English and German UD\n",
        "# # corpus: MultiCorpus = MultiCorpus([UD_ENGLISH(), UD_GERMAN()]).downsample(0.1)\n",
        "# corpus: Corpus = UD_ENGLISH().downsample(0.1)\n",
        "# print(type(corpus.train))\n",
        "# print(type(corpus.train[0]))\n",
        "\n",
        "\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "# define columns\n",
        "columns = {0: 'text', 1: 'pos'}\n",
        "\n",
        "# this is the folder in which train, test and dev files reside\n",
        "data_folder =   ''  #'/path/to/data/folder'\n",
        "\n",
        "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='fulldata_train.txt',\n",
        "                              test_file='fulldata_test.txt',\n",
        "                              dev_file='fulldata_dev.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-14 22:46:58,563 Reading data from .\n",
            "2020-01-14 22:46:58,565 Train: fulldata_train.txt\n",
            "2020-01-14 22:46:58,566 Dev: fulldata_dev.txt\n",
            "2020-01-14 22:46:58,567 Test: fulldata_test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yNhR0XLCRC9",
        "colab_type": "code",
        "outputId": "9df64cbc-2617-41f1-e87e-8555a73a3a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(corpus.train))\n",
        "print(len(corpus.test))\n",
        "print(len(corpus.dev))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4245\n",
            "735\n",
            "678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN7lcItTCUBc",
        "colab_type": "code",
        "outputId": "3fd891dc-6532-44d3-91c8-118823199f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(corpus.test[1].to_tagged_string('pos'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I <PRP> was <VBD> in <IN> my <PRP$> 20s <NNS> before <IN> I <PRP> ever <RB> went <VBD> to <TO> an <DT> art <NN> museum <NN> . <.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGQMqndQZlQY",
        "colab_type": "code",
        "outputId": "2c0989f0-94f5-4f92-a86b-91b8da030c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#TRAINING + TESTING\n",
        "\n",
        "# 2. what tag do we want to predict?\n",
        "# tag_type = 'pos-fast'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type='pos')\n",
        "print(tag_dictionary.idx2item)\n",
        "\n",
        "# 4. initialize embeddings\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "\n",
        "    # we use multilingual Flair embeddings in this task\n",
        "    FlairEmbeddings('multi-forward'),\n",
        "    FlairEmbeddings('multi-backward'),\n",
        "    # WordEmbeddings('glove'),\n",
        "    # CharacterEmbeddings(),\n",
        "\n",
        "    \n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type='pos',\n",
        "                                        use_crf=False)\n",
        "\n",
        "# 6. initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "# 7. start training\n",
        "trainer.train('resources/taggers/example-universal-pos',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=64,\n",
        "              max_epochs=10,\n",
        "              checkpoint=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[b'<unk>', b'O', b'DT', b'NN', b'VBZ', b'WRB', b'EX', b'.', b'``', b'PRP', b'VBD', b',', b'VBG', b'IN', b'JJS', b'NNP', b'POS', b'NNS', b'JJ', b'CC', b'RB', b':', b'WP', b'TO', b'VB', b'-LRB-', b'-RRB-', b'PDT', b'VBP', b'PRP$', b'VBN', b'WDT', b'CD', b'MD', b'RBR', b'NNPS', b'UH', b'JJR', b'RP', b'RBS', b'SYM', b'LS', b'FW', b'$', b'WP$', b'<START>', b'<STOP>']\n",
            "2020-01-14 22:47:29,440 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-forward-v0.1.pt not found in cache, downloading to /tmp/tmp7_6yci11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 73034300/73034300 [00:01<00:00, 70888350.46B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-14 22:47:30,650 copying /tmp/tmp7_6yci11 to cache at /root/.flair/embeddings/lm-multi-forward-v0.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-14 22:47:30,746 removing temp file /tmp/tmp7_6yci11\n",
            "2020-01-14 22:47:32,473 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-backward-v0.1.pt not found in cache, downloading to /tmp/tmp1jenvo1u\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 73034304/73034304 [00:01<00:00, 62109068.25B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-14 22:47:33,846 copying /tmp/tmp1jenvo1u to cache at /root/.flair/embeddings/lm-multi-backward-v0.1.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-14 22:47:33,954 removing temp file /tmp/tmp1jenvo1u\n",
            "2020-01-14 22:47:34,493 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-14 22:47:34,499 Evaluation method: MICRO_F1_SCORE\n",
            "2020-01-14 22:47:34,804 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-14 22:48:36,425 epoch 1 - iter 0/67 - loss 3.85884857\n",
            "2020-01-14 22:52:56,152 epoch 1 - iter 6/67 - loss 3.77065778\n",
            "2020-01-14 22:56:16,210 epoch 1 - iter 12/67 - loss 3.68004120\n",
            "2020-01-14 23:01:14,898 epoch 1 - iter 18/67 - loss 3.58882081\n",
            "2020-01-14 23:05:02,813 epoch 1 - iter 24/67 - loss 3.49475676\n",
            "2020-01-14 23:08:46,029 epoch 1 - iter 30/67 - loss 3.42037627\n",
            "2020-01-14 23:12:31,248 epoch 1 - iter 36/67 - loss 3.35648774\n",
            "2020-01-14 23:16:20,880 epoch 1 - iter 42/67 - loss 3.29641362\n",
            "2020-01-14 23:20:38,533 epoch 1 - iter 48/67 - loss 3.24001138\n",
            "2020-01-14 23:25:20,477 epoch 1 - iter 54/67 - loss 3.19091219\n",
            "2020-01-14 23:28:42,004 epoch 1 - iter 60/67 - loss 3.14388811\n",
            "2020-01-14 23:31:53,579 epoch 1 - iter 66/67 - loss 3.09943443\n",
            "2020-01-14 23:31:53,648 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-14 23:31:53,650 EPOCH 1 done: loss 3.0994 - lr 0.1000 - bad epochs 0\n",
            "2020-01-14 23:38:01,308 DEV : loss 2.5697457790374756 - score 0.3969\n",
            "2020-01-14 23:44:26,903 TEST : loss 2.555361032485962 - score 0.3991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-14 23:44:28,018 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-14 23:45:02,915 epoch 2 - iter 0/67 - loss 2.55677319\n",
            "2020-01-14 23:48:37,960 epoch 2 - iter 6/67 - loss 2.58613634\n",
            "2020-01-14 23:53:07,144 epoch 2 - iter 12/67 - loss 2.53922362\n",
            "2020-01-14 23:57:16,184 epoch 2 - iter 18/67 - loss 2.52094782\n",
            "2020-01-15 00:01:59,865 epoch 2 - iter 24/67 - loss 2.48403743\n",
            "2020-01-15 00:05:43,142 epoch 2 - iter 30/67 - loss 2.45040527\n",
            "2020-01-15 00:08:54,262 epoch 2 - iter 36/67 - loss 2.40988188\n",
            "2020-01-15 00:12:49,030 epoch 2 - iter 42/67 - loss 2.37795304\n",
            "2020-01-15 00:16:04,066 epoch 2 - iter 48/67 - loss 2.34261633\n",
            "2020-01-15 00:19:41,402 epoch 2 - iter 54/67 - loss 2.31469783\n",
            "2020-01-15 00:24:09,964 epoch 2 - iter 60/67 - loss 2.28195297\n",
            "2020-01-15 00:27:57,752 epoch 2 - iter 66/67 - loss 2.24593109\n",
            "2020-01-15 00:27:58,041 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 00:27:58,043 EPOCH 2 done: loss 2.2459 - lr 0.1000 - bad epochs 0\n",
            "2020-01-15 00:34:07,727 DEV : loss 1.7860544919967651 - score 0.5337\n",
            "2020-01-15 00:40:39,285 TEST : loss 1.7673344612121582 - score 0.532\n",
            "2020-01-15 00:40:40,579 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 00:41:12,861 epoch 3 - iter 0/67 - loss 2.02318001\n",
            "2020-01-15 00:44:46,437 epoch 3 - iter 6/67 - loss 1.92607587\n",
            "2020-01-15 00:48:31,527 epoch 3 - iter 12/67 - loss 1.85235087\n",
            "2020-01-15 00:52:44,275 epoch 3 - iter 18/67 - loss 1.81516583\n",
            "2020-01-15 00:55:50,973 epoch 3 - iter 24/67 - loss 1.78751752\n",
            "2020-01-15 01:00:31,062 epoch 3 - iter 30/67 - loss 1.76577301\n",
            "2020-01-15 01:04:01,963 epoch 3 - iter 36/67 - loss 1.74342681\n",
            "2020-01-15 01:08:04,988 epoch 3 - iter 42/67 - loss 1.71746521\n",
            "2020-01-15 01:11:34,172 epoch 3 - iter 48/67 - loss 1.70141546\n",
            "2020-01-15 01:16:31,156 epoch 3 - iter 54/67 - loss 1.68708202\n",
            "2020-01-15 01:20:40,841 epoch 3 - iter 60/67 - loss 1.66752982\n",
            "2020-01-15 01:23:47,514 epoch 3 - iter 66/67 - loss 1.64510920\n",
            "2020-01-15 01:23:47,829 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 01:23:47,830 EPOCH 3 done: loss 1.6451 - lr 0.1000 - bad epochs 0\n",
            "2020-01-15 01:30:03,154 DEV : loss 1.272640585899353 - score 0.7091\n",
            "2020-01-15 01:36:33,834 TEST : loss 1.2489088773727417 - score 0.7094\n",
            "2020-01-15 01:36:35,031 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 01:37:22,703 epoch 4 - iter 0/67 - loss 1.53858423\n",
            "2020-01-15 01:40:38,368 epoch 4 - iter 6/67 - loss 1.43349160\n",
            "2020-01-15 01:43:54,676 epoch 4 - iter 12/67 - loss 1.40319973\n",
            "2020-01-15 01:47:27,898 epoch 4 - iter 18/67 - loss 1.38160481\n",
            "2020-01-15 01:52:02,428 epoch 4 - iter 24/67 - loss 1.36665657\n",
            "2020-01-15 01:57:10,129 epoch 4 - iter 30/67 - loss 1.36421184\n",
            "2020-01-15 02:01:25,655 epoch 4 - iter 36/67 - loss 1.34885757\n",
            "2020-01-15 02:04:50,179 epoch 4 - iter 42/67 - loss 1.33460410\n",
            "2020-01-15 02:09:40,709 epoch 4 - iter 48/67 - loss 1.31074213\n",
            "2020-01-15 02:13:38,135 epoch 4 - iter 54/67 - loss 1.30052090\n",
            "2020-01-15 02:17:19,171 epoch 4 - iter 60/67 - loss 1.28585066\n",
            "2020-01-15 02:20:29,583 epoch 4 - iter 66/67 - loss 1.27314486\n",
            "2020-01-15 02:20:29,897 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 02:20:29,898 EPOCH 4 done: loss 1.2731 - lr 0.1000 - bad epochs 0\n",
            "2020-01-15 02:26:43,592 DEV : loss 0.9387841820716858 - score 0.7768\n",
            "2020-01-15 02:33:15,910 TEST : loss 0.9121764302253723 - score 0.7792\n",
            "2020-01-15 02:33:17,205 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 02:34:04,735 epoch 5 - iter 0/67 - loss 1.02463222\n",
            "2020-01-15 02:37:45,637 epoch 5 - iter 6/67 - loss 1.09510211\n",
            "2020-01-15 02:41:11,743 epoch 5 - iter 12/67 - loss 1.10200426\n",
            "2020-01-15 02:45:02,115 epoch 5 - iter 18/67 - loss 1.08507352\n",
            "2020-01-15 02:48:46,825 epoch 5 - iter 24/67 - loss 1.07403175\n",
            "2020-01-15 02:52:22,630 epoch 5 - iter 30/67 - loss 1.07324792\n",
            "2020-01-15 02:57:21,650 epoch 5 - iter 36/67 - loss 1.05576684\n",
            "2020-01-15 03:01:25,718 epoch 5 - iter 42/67 - loss 1.04292055\n",
            "2020-01-15 03:05:12,940 epoch 5 - iter 48/67 - loss 1.03488006\n",
            "2020-01-15 03:08:27,610 epoch 5 - iter 54/67 - loss 1.02179697\n",
            "2020-01-15 03:11:53,860 epoch 5 - iter 60/67 - loss 1.01298632\n",
            "2020-01-15 03:16:29,085 epoch 5 - iter 66/67 - loss 1.00235385\n",
            "2020-01-15 03:16:29,407 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 03:16:29,412 EPOCH 5 done: loss 1.0024 - lr 0.1000 - bad epochs 0\n",
            "2020-01-15 03:22:42,698 DEV : loss 0.7265768647193909 - score 0.8181\n",
            "2020-01-15 03:29:13,771 TEST : loss 0.6975147724151611 - score 0.8248\n",
            "2020-01-15 03:29:15,065 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 03:30:05,666 epoch 6 - iter 0/67 - loss 1.13807833\n",
            "2020-01-15 03:33:42,227 epoch 6 - iter 6/67 - loss 0.93780048\n",
            "2020-01-15 03:37:11,776 epoch 6 - iter 12/67 - loss 0.91885149\n",
            "2020-01-15 03:40:32,916 epoch 6 - iter 18/67 - loss 0.90390704\n",
            "2020-01-15 03:44:42,368 epoch 6 - iter 24/67 - loss 0.90244219\n",
            "2020-01-15 03:48:48,704 epoch 6 - iter 30/67 - loss 0.89332410\n",
            "2020-01-15 03:52:21,006 epoch 6 - iter 36/67 - loss 0.87787487\n",
            "2020-01-15 03:56:27,491 epoch 6 - iter 42/67 - loss 0.87830600\n",
            "2020-01-15 03:59:45,393 epoch 6 - iter 48/67 - loss 0.87509492\n",
            "2020-01-15 04:04:35,729 epoch 6 - iter 54/67 - loss 0.86902642\n",
            "2020-01-15 04:09:31,630 epoch 6 - iter 60/67 - loss 0.86861793\n",
            "2020-01-15 04:12:29,640 epoch 6 - iter 66/67 - loss 0.86525945\n",
            "2020-01-15 04:12:29,974 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 04:12:29,977 EPOCH 6 done: loss 0.8653 - lr 0.1000 - bad epochs 0\n",
            "2020-01-15 04:18:42,625 DEV : loss 0.5967475175857544 - score 0.851\n",
            "2020-01-15 04:25:11,000 TEST : loss 0.5681366920471191 - score 0.8558\n",
            "2020-01-15 04:25:12,338 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 04:25:45,127 epoch 7 - iter 0/67 - loss 0.78093004\n",
            "2020-01-15 04:29:29,703 epoch 7 - iter 6/67 - loss 0.81221811\n",
            "2020-01-15 04:33:20,708 epoch 7 - iter 12/67 - loss 0.79694920\n",
            "2020-01-15 04:37:54,946 epoch 7 - iter 18/67 - loss 0.78709974\n",
            "2020-01-15 04:41:43,072 epoch 7 - iter 24/67 - loss 0.77685838\n",
            "2020-01-15 04:45:11,769 epoch 7 - iter 30/67 - loss 0.77344811\n",
            "2020-01-15 04:49:38,857 epoch 7 - iter 36/67 - loss 0.77282401\n",
            "2020-01-15 04:53:30,485 epoch 7 - iter 42/67 - loss 0.77360566\n",
            "2020-01-15 04:57:53,982 epoch 7 - iter 48/67 - loss 0.78055018\n",
            "2020-01-15 05:01:40,463 epoch 7 - iter 54/67 - loss 0.77355872\n",
            "2020-01-15 05:05:31,729 epoch 7 - iter 60/67 - loss 0.76228829\n",
            "2020-01-15 05:09:31,578 epoch 7 - iter 66/67 - loss 0.76388982\n",
            "2020-01-15 05:09:31,954 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 05:09:31,956 EPOCH 7 done: loss 0.7639 - lr 0.1000 - bad epochs 0\n",
            "2020-01-15 05:15:48,855 DEV : loss 0.5033760070800781 - score 0.8695\n",
            "2020-01-15 05:22:18,905 TEST : loss 0.47769951820373535 - score 0.876\n",
            "2020-01-15 05:22:20,301 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 05:23:08,715 epoch 8 - iter 0/67 - loss 0.78059143\n",
            "2020-01-15 05:27:23,765 epoch 8 - iter 6/67 - loss 0.69526413\n",
            "2020-01-15 05:31:41,123 epoch 8 - iter 12/67 - loss 0.71894739\n",
            "2020-01-15 05:35:20,220 epoch 8 - iter 18/67 - loss 0.70263350\n",
            "2020-01-15 05:39:09,145 epoch 8 - iter 24/67 - loss 0.70618395\n",
            "2020-01-15 05:42:30,579 epoch 8 - iter 30/67 - loss 0.69823754\n",
            "2020-01-15 05:46:25,710 epoch 8 - iter 36/67 - loss 0.69294630\n",
            "2020-01-15 05:51:02,739 epoch 8 - iter 42/67 - loss 0.68779562\n",
            "2020-01-15 05:55:43,936 epoch 8 - iter 48/67 - loss 0.69140186\n",
            "2020-01-15 05:59:48,126 epoch 8 - iter 54/67 - loss 0.68469744\n",
            "2020-01-15 06:03:25,096 epoch 8 - iter 60/67 - loss 0.69140655\n",
            "2020-01-15 06:07:35,488 epoch 8 - iter 66/67 - loss 0.69231848\n",
            "2020-01-15 06:07:35,879 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 06:07:35,881 EPOCH 8 done: loss 0.6923 - lr 0.1000 - bad epochs 0\n",
            "2020-01-15 06:13:55,395 DEV : loss 0.43291881680488586 - score 0.8844\n",
            "2020-01-15 06:20:27,125 TEST : loss 0.4066460132598877 - score 0.891\n",
            "2020-01-15 06:20:28,390 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 06:21:16,073 epoch 9 - iter 0/67 - loss 0.48506388\n",
            "2020-01-15 06:25:18,024 epoch 9 - iter 6/67 - loss 0.59625325\n",
            "2020-01-15 06:28:58,166 epoch 9 - iter 12/67 - loss 0.62598879\n",
            "2020-01-15 06:33:04,916 epoch 9 - iter 18/67 - loss 0.64700062\n",
            "2020-01-15 06:36:38,754 epoch 9 - iter 24/67 - loss 0.64057899\n",
            "2020-01-15 06:41:06,811 epoch 9 - iter 30/67 - loss 0.62639629\n",
            "2020-01-15 06:45:10,992 epoch 9 - iter 36/67 - loss 0.63341609\n",
            "2020-01-15 06:48:54,946 epoch 9 - iter 42/67 - loss 0.62436982\n",
            "2020-01-15 06:54:09,628 epoch 9 - iter 48/67 - loss 0.62351954\n",
            "2020-01-15 06:58:21,360 epoch 9 - iter 54/67 - loss 0.61527753\n",
            "2020-01-15 07:02:10,190 epoch 9 - iter 60/67 - loss 0.62024013\n",
            "2020-01-15 07:05:05,186 epoch 9 - iter 66/67 - loss 0.61379149\n",
            "2020-01-15 07:05:05,595 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 07:05:05,597 EPOCH 9 done: loss 0.6138 - lr 0.1000 - bad epochs 0\n",
            "2020-01-15 07:11:18,604 DEV : loss 0.38380804657936096 - score 0.8969\n",
            "2020-01-15 07:17:53,673 TEST : loss 0.36248326301574707 - score 0.9027\n",
            "2020-01-15 07:17:54,996 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 07:18:32,195 epoch 10 - iter 0/67 - loss 0.53596443\n",
            "2020-01-15 07:22:36,042 epoch 10 - iter 6/67 - loss 0.59747193\n",
            "2020-01-15 07:26:41,479 epoch 10 - iter 12/67 - loss 0.61294987\n",
            "2020-01-15 07:31:07,617 epoch 10 - iter 18/67 - loss 0.59136070\n",
            "2020-01-15 07:35:04,075 epoch 10 - iter 24/67 - loss 0.59646501\n",
            "2020-01-15 07:38:54,318 epoch 10 - iter 30/67 - loss 0.60573296\n",
            "2020-01-15 07:42:34,133 epoch 10 - iter 36/67 - loss 0.59134747\n",
            "2020-01-15 07:46:38,085 epoch 10 - iter 42/67 - loss 0.59464191\n",
            "2020-01-15 07:50:49,324 epoch 10 - iter 48/67 - loss 0.59033935\n",
            "2020-01-15 07:54:54,543 epoch 10 - iter 54/67 - loss 0.57946677\n",
            "2020-01-15 07:58:32,383 epoch 10 - iter 60/67 - loss 0.57755050\n",
            "2020-01-15 08:02:12,363 epoch 10 - iter 66/67 - loss 0.56332061\n",
            "2020-01-15 08:02:12,723 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 08:02:12,724 EPOCH 10 done: loss 0.5633 - lr 0.1000 - bad epochs 0\n",
            "2020-01-15 08:08:30,047 DEV : loss 0.35208120942115784 - score 0.9043\n",
            "2020-01-15 08:15:07,102 TEST : loss 0.33009159564971924 - score 0.9097\n",
            "2020-01-15 08:15:09,470 ----------------------------------------------------------------------------------------------------\n",
            "2020-01-15 08:15:09,476 Testing using best model ...\n",
            "2020-01-15 08:15:09,483 loading file resources/taggers/example-universal-pos/best-model.pt\n",
            "2020-01-15 08:21:53,994 0.9097\t0.9097\t0.9097\n",
            "2020-01-15 08:21:53,996 \n",
            "MICRO_AVG: acc 0.8344 - f1-score 0.9097\n",
            "MACRO_AVG: acc 0.5126 - f1-score 0.5699095238095238\n",
            "$          tp: 0 - fp: 0 - fn: 6 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            ",          tp: 607 - fp: 89 - fn: 1 - tn: 607 - precision: 0.8721 - recall: 0.9984 - accuracy: 0.8709 - f1-score: 0.9310\n",
            "-LRB-      tp: 0 - fp: 0 - fn: 15 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "-RRB-      tp: 0 - fp: 0 - fn: 13 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            ".          tp: 733 - fp: 4 - fn: 2 - tn: 733 - precision: 0.9946 - recall: 0.9973 - accuracy: 0.9919 - f1-score: 0.9959\n",
            ":          tp: 15 - fp: 2 - fn: 85 - tn: 15 - precision: 0.8824 - recall: 0.1500 - accuracy: 0.1471 - f1-score: 0.2564\n",
            "CC         tp: 548 - fp: 5 - fn: 18 - tn: 548 - precision: 0.9910 - recall: 0.9682 - accuracy: 0.9597 - f1-score: 0.9795\n",
            "CD         tp: 136 - fp: 20 - fn: 45 - tn: 136 - precision: 0.8718 - recall: 0.7514 - accuracy: 0.6766 - f1-score: 0.8071\n",
            "DT         tp: 1323 - fp: 100 - fn: 22 - tn: 1323 - precision: 0.9297 - recall: 0.9836 - accuracy: 0.9156 - f1-score: 0.9559\n",
            "EX         tp: 0 - fp: 0 - fn: 23 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "FW         tp: 0 - fp: 0 - fn: 3 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "IN         tp: 1477 - fp: 115 - fn: 64 - tn: 1477 - precision: 0.9278 - recall: 0.9585 - accuracy: 0.8919 - f1-score: 0.9429\n",
            "JJ         tp: 681 - fp: 144 - fn: 118 - tn: 681 - precision: 0.8255 - recall: 0.8523 - accuracy: 0.7222 - f1-score: 0.8387\n",
            "JJR        tp: 0 - fp: 0 - fn: 28 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "JJS        tp: 0 - fp: 0 - fn: 16 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "MD         tp: 209 - fp: 7 - fn: 11 - tn: 209 - precision: 0.9676 - recall: 0.9500 - accuracy: 0.9207 - f1-score: 0.9587\n",
            "NN         tp: 1670 - fp: 121 - fn: 159 - tn: 1670 - precision: 0.9324 - recall: 0.9131 - accuracy: 0.8564 - f1-score: 0.9226\n",
            "NNP        tp: 630 - fp: 161 - fn: 28 - tn: 630 - precision: 0.7965 - recall: 0.9574 - accuracy: 0.7692 - f1-score: 0.8696\n",
            "NNPS       tp: 0 - fp: 0 - fn: 16 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "NNS        tp: 630 - fp: 29 - fn: 26 - tn: 630 - precision: 0.9560 - recall: 0.9604 - accuracy: 0.9197 - f1-score: 0.9582\n",
            "PDT        tp: 0 - fp: 0 - fn: 10 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "POS        tp: 26 - fp: 0 - fn: 24 - tn: 26 - precision: 1.0000 - recall: 0.5200 - accuracy: 0.5200 - f1-score: 0.6842\n",
            "PRP        tp: 957 - fp: 39 - fn: 3 - tn: 957 - precision: 0.9608 - recall: 0.9969 - accuracy: 0.9580 - f1-score: 0.9785\n",
            "PRP$       tp: 0 - fp: 0 - fn: 44 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "RB         tp: 586 - fp: 98 - fn: 108 - tn: 586 - precision: 0.8567 - recall: 0.8444 - accuracy: 0.7399 - f1-score: 0.8505\n",
            "RBR        tp: 0 - fp: 0 - fn: 15 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "RBS        tp: 0 - fp: 0 - fn: 6 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "RP         tp: 41 - fp: 15 - fn: 31 - tn: 41 - precision: 0.7321 - recall: 0.5694 - accuracy: 0.4713 - f1-score: 0.6406\n",
            "SYM        tp: 0 - fp: 0 - fn: 1 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "TO         tp: 332 - fp: 29 - fn: 7 - tn: 332 - precision: 0.9197 - recall: 0.9794 - accuracy: 0.9022 - f1-score: 0.9486\n",
            "UH         tp: 57 - fp: 20 - fn: 14 - tn: 57 - precision: 0.7403 - recall: 0.8028 - accuracy: 0.6264 - f1-score: 0.7703\n",
            "VB         tp: 593 - fp: 89 - fn: 20 - tn: 593 - precision: 0.8695 - recall: 0.9674 - accuracy: 0.8447 - f1-score: 0.9158\n",
            "VBD        tp: 495 - fp: 68 - fn: 50 - tn: 495 - precision: 0.8792 - recall: 0.9083 - accuracy: 0.8075 - f1-score: 0.8935\n",
            "VBG        tp: 206 - fp: 14 - fn: 48 - tn: 206 - precision: 0.9364 - recall: 0.8110 - accuracy: 0.7687 - f1-score: 0.8692\n",
            "VBN        tp: 239 - fp: 33 - fn: 75 - tn: 239 - precision: 0.8787 - recall: 0.7611 - accuracy: 0.6888 - f1-score: 0.8157\n",
            "VBP        tp: 350 - fp: 34 - fn: 46 - tn: 350 - precision: 0.9115 - recall: 0.8838 - accuracy: 0.8140 - f1-score: 0.8974\n",
            "VBZ        tp: 284 - fp: 30 - fn: 36 - tn: 284 - precision: 0.9045 - recall: 0.8875 - accuracy: 0.8114 - f1-score: 0.8959\n",
            "WDT        tp: 87 - fp: 9 - fn: 23 - tn: 87 - precision: 0.9062 - recall: 0.7909 - accuracy: 0.7311 - f1-score: 0.8446\n",
            "WP         tp: 66 - fp: 13 - fn: 6 - tn: 66 - precision: 0.8354 - recall: 0.9167 - accuracy: 0.7765 - f1-score: 0.8742\n",
            "WP$        tp: 0 - fp: 0 - fn: 1 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "WRB        tp: 44 - fp: 3 - fn: 35 - tn: 44 - precision: 0.9362 - recall: 0.5570 - accuracy: 0.5366 - f1-score: 0.6985\n",
            "``         tp: 106 - fp: 12 - fn: 1 - tn: 106 - precision: 0.8983 - recall: 0.9907 - accuracy: 0.8908 - f1-score: 0.9422\n",
            "2020-01-15 08:21:53,997 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [tensor(2.5697),\n",
              "  tensor(1.7861),\n",
              "  tensor(1.2726),\n",
              "  tensor(0.9388),\n",
              "  tensor(0.7266),\n",
              "  tensor(0.5967),\n",
              "  tensor(0.5034),\n",
              "  tensor(0.4329),\n",
              "  tensor(0.3838),\n",
              "  tensor(0.3521)],\n",
              " 'dev_score_history': [0.3969,\n",
              "  0.5337,\n",
              "  0.7091,\n",
              "  0.7768,\n",
              "  0.8181,\n",
              "  0.851,\n",
              "  0.8695,\n",
              "  0.8844,\n",
              "  0.8969,\n",
              "  0.9043],\n",
              " 'test_score': 0.9097,\n",
              " 'train_loss_history': [3.099434429140233,\n",
              "  2.2459310898140297,\n",
              "  1.645109203324389,\n",
              "  1.273144857207341,\n",
              "  1.0023538479164464,\n",
              "  0.865259447204533,\n",
              "  0.7638898180491889,\n",
              "  0.6923184804062346,\n",
              "  0.6137914879998164,\n",
              "  0.563320611395053]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q1q95oEQJ1H",
        "colab_type": "code",
        "outputId": "ac44cc82-4ebd-4275-f04d-aeb6b10a2108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# # For no loss in model learned  + to resuming trainin form where we left\n",
        "# from pathlib import Path\n",
        "\n",
        "# checkpoint = 'resources/taggers/example-universal-pos/checkpoint.pt'\n",
        "# trainer = ModelTrainer.load_checkpoint(checkpoint, corpus)\n",
        "# trainer.train('resources/taggers/example-universal-pos',\n",
        "#               learning_rate=0.1,\n",
        "#               mini_batch_size=32,\n",
        "#               max_epochs=10,\n",
        "#               checkpoint=True) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e3002f1a455a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'resources/taggers/example-universal-pos/checkpoint.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m trainer.train('resources/taggers/example-universal-pos',\n\u001b[1;32m      6\u001b[0m               \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'ModelTrainer' has no attribute 'load_checkpoint'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLNjRZuJ5xax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PLOTING LOSS\n",
        "from flair.visual.training_curves import Plotter\n",
        "plotter = Plotter()\n",
        "plotter.plot_training_curves('resources/taggers/example-universal-pos/loss.tsv')\n",
        "# plotter.plot_weights('weights.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_se0bIdApYP",
        "colab_type": "code",
        "outputId": "aa4d59e9-d610-47f3-9a29-5a3cfb878210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#TESTING  - BUT JUST A SENTENCE\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "\n",
        "model = SequenceTagger.load('resources/taggers/example-universal-pos/final-model.pt')\n",
        "m = SequenceTagger.load('pos-fast')\n",
        "\n",
        "# create example sentence\n",
        "sentence = Sentence('France is the current world cup winner .')\n",
        "s2 = Sentence('France is the current world cup winner .')\n",
        "\n",
        "# predict class and print\n",
        "model.predict(sentence)\n",
        "m.predict(s2)\n",
        "\n",
        "print(sentence.to_tagged_string())\n",
        "print(s2.to_tagged_string())\n",
        "# print(sentence.tokens)\n",
        "# print(classifier.sentence.to_tagged_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-14 18:04:00,290 loading file resources/taggers/example-universal-pos/final-model.pt\n",
            "2020-01-14 18:04:01,831 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.2/POS-ontonotes--h256-l1-b32-%2Bnews-forward-fast%2Bnews-backward-fast--v0.2/en-pos-ontonotes-fast-v0.2.pt not found in cache, downloading to /tmp/tmpha45pdpz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75210112/75210112 [00:09<00:00, 8096954.85B/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-14 18:04:12,314 copying /tmp/tmpha45pdpz to cache at /root/.flair/models/en-pos-ontonotes-fast-v0.2.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-14 18:04:12,436 removing temp file /tmp/tmpha45pdpz\n",
            "2020-01-14 18:04:12,920 loading file /root/.flair/models/en-pos-ontonotes-fast-v0.2.pt\n",
            "France <NN> is <IN> the <DT> current <NN> world <NN> cup <NN> winner <NN> . <NN>\n",
            "France <NNP> is <VBZ> the <DT> current <JJ> world <NN> cup <NN> winner <NN> . <.>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}